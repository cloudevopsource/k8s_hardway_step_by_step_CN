kube-proxy 运行在所有 worker 节点上，它监听 apiserver 中 service 和 endpoint 的变化情况，创建路由规则以提供服务 IP 和负载均衡功能。

本文档讲解部署 ipvs 模式的 kube-proxy 过程。

注意：如果没有特殊指明，本文档的所有操作**均在 zhangjun-k8s-01 节点上执行**，然后远程分发文件和执行命令。



```

```

## 创建 kube-proxy 配置文件

从 v1.10 开始，kube-proxy **部分参数**可以配置文件中配置。可以使用 `--write-config-to` 选项生成该配置文件，或者参考 [源代码的注释](https://github.com/kubernetes/kubernetes/blob/release-1.14/pkg/proxy/apis/config/types.go)。

创建 kube-proxy config 文件模板：

```
cd /usr/local/kubernetes/work
source /usr/local/kubernetes/bin/environment.sh
cat > kube-proxy-config.yaml.template <<EOF
kind: KubeProxyConfiguration
apiVersion: kubeproxy.config.k8s.io/v1alpha1
bindAddress: 0.0.0.0
clientConnection:
  kubeconfig: "/etc/kubernetes/kube-proxy.kubeconfig"
# 根据clusterCIDR 判断集群内部和外部流量，配置clusterCIDR选项后，kube-proxy 会对访问 Service IP 的请求做 SNAT
clusterCIDR: ${CLUSTER_CIDR}
conntrack:
  maxPerCore: 32768
  min: 131072
  tcpCloseWaitTimeout: 1h0m0s
  tcpEstablishedTimeout: 24h0m0s
healthzBindAddress: 0.0.0.0:10256
# hostnameOverride 值必须与 kubelet 的对应一致，否则 kube-proxy 启动后会找不到该 Node，从而不会创建任何 iptables 规则
hostnameOverride: ##NODE_NAME##
metricsBindAddress: 0.0.0.0:10249
mode: ipvs
EOF
```

- `bindAddress`: 监听地址；
- `clientConnection.kubeconfig`: 连接 apiserver 的 kubeconfig 文件；
- `clusterCIDR`: kube-proxy 根据 `--cluster-cidr` 判断集群内部和外部流量，指定 `--cluster-cidr` 或 `--masquerade-all` 选项后 kube-proxy 才会对访问 Service IP 的请求做 SNAT；
- `hostnameOverride`: 参数值必须与 kubelet 的值一致，否则 kube-proxy 启动后会找不到该 Node，从而不会创建任何 ipvs 规则；
- `mode`: 使用 ipvs 模式；

为各节点创建和分发 kube-proxy 配置文件：

```
cd /usr/local/kubernetes/work
source /usr/local/kubernetes/bin/environment.sh
for (( i=0; i < 5; i++ ))
  do 
    echo ">>> ${NODE_NAMES[i]}"
    sed -e "s/##NODE_NAME##/${NODE_NAMES[i]}/" -e "s/##NODE_IP##/${NODE_IPS[i]}/" kube-proxy-config.yaml.template > kube-proxy-config-${NODE_NAMES[i]}.yaml.template
    scp kube-proxy-config-${NODE_NAMES[i]}.yaml.template root@${NODE_NAMES[i]}:/etc/kubernetes/kube-proxy-${NODE_NAMES[i]}-config.yaml
  done
```

## 创建和分发 kube-proxy systemd unit 文件

```
cd /usr/local/kubernetes/work
source /usr/local/k8s/kubernetes/environment.sh
cat > kube-proxy.service.template <<EOF
[Unit]
Description=Kubernetes Kube-Proxy Server
Documentation=https://github.com/GoogleCloudPlatform/kubernetes
After=network.target

[Service]
WorkingDirectory=/data/k8s/k8s/kube-proxy
ExecStart=/usr/local/kubernetes/bin/kube-proxy \\
  --config=/etc/kubernetes/kube-proxy-##NODE_NAME##-config.yaml \\
  --logtostderr=true \\
  --v=2
Restart=on-failure
RestartSec=5
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target

EOF
```

分发 kube-proxy systemd unit 文件：

```
cd /usr/local/kubernetes/work
source /usr/local/kubernetes/bin/environment.sh
for (( i=0; i < 5; i++ ))
  do
    echo ">>> ${NODE_IPS[i]}"
    sed -e "s/##NODE_NAME##/${NODE_NAMES[i]}/" kube-proxy.service.template > kube-proxy-${NODE_NAMES[i]}.service 
    scp kube-proxy-${NODE_NAMES[i]}.service root@${NODE_IPS[i]}:/etc/systemd/system/kube-proxy.service
  done
```

## 启动 kube-proxy 服务

```
cd /usr/local/kubernetes/work
source /usr/local/kubernetes/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo ">>> ${node_ip}"
    ssh root@${node_ip} "systemctl daemon-reload && systemctl enable kube-proxy && systemctl restart kube-proxy"
  done
```

## 检查启动结果

```
cd /usr/local/kubernetes/work
source /usr/local/kubernetes/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo ">>> ${node_ip}"
    ssh root@${node_ip} "systemctl status kube-proxy|grep Active"
  done
```

确保状态为 `active (running)`，否则查看日志，确认原因：

```
journalctl -u kube-proxy
```

## 查看监听端口

```
[root@k8scloud1 work]# sudo netstat -lnpt|grep kube-prox
tcp        0      0 192.168.224.1:10249     0.0.0.0:*               LISTEN      11715/kube-proxy    
tcp        0      0 192.168.224.1:10256     0.0.0.0:*               LISTEN      11715/kube-proxy 
```

- 10249：http prometheus metrics port;
- 10256：http healthz port;

## 查看 ipvs 路由规则

```
source /usr/local/kubernetes/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo ">>> ${node_ip}"
    ssh root@${node_ip} "/usr/sbin/ipvsadm -ln"
  done
```

预期输出：

```
[root@tsk8s-master1 work]# for node_ip in ${NODE_IPS[@]}
>   do
>     echo ">>> ${node_ip}"
>     ssh root@${node_ip} "/usr/sbin/ipvsadm -ln"
>   done
>>> 172.16.32.211
IP Virtual Server version 1.2.1 (size=4096)
Prot LocalAddress:Port Scheduler Flags
  -> RemoteAddress:Port           Forward Weight ActiveConn InActConn
TCP  10.254.0.1:443 rr
  -> 172.16.32.211:6443           Masq    1      0          0
  -> 172.16.32.212:6443           Masq    1      0          0
  -> 172.16.32.213:6443           Masq    1      0          0
>>> 172.16.32.212
IP Virtual Server version 1.2.1 (size=4096)
Prot LocalAddress:Port Scheduler Flags
  -> RemoteAddress:Port           Forward Weight ActiveConn InActConn
TCP  10.254.0.1:443 rr
  -> 172.16.32.211:6443           Masq    1      0          0
  -> 172.16.32.212:6443           Masq    1      0          0
  -> 172.16.32.213:6443           Masq    1      0          0
>>> 172.16.32.213
IP Virtual Server version 1.2.1 (size=4096)
Prot LocalAddress:Port Scheduler Flags
  -> RemoteAddress:Port           Forward Weight ActiveConn InActConn
TCP  10.254.0.1:443 rr
  -> 172.16.32.211:6443           Masq    1      0          0
  -> 172.16.32.212:6443           Masq    1      0          0
  -> 172.16.32.213:6443           Masq    1      0          0
>>> 172.16.32.215
IP Virtual Server version 1.2.1 (size=4096)
Prot LocalAddress:Port Scheduler Flags
  -> RemoteAddress:Port           Forward Weight ActiveConn InActConn
TCP  10.254.0.1:443 rr
  -> 172.16.32.211:6443           Masq    1      0          0
  -> 172.16.32.212:6443           Masq    1      0          0
  -> 172.16.32.213:6443           Masq    1      0          0
>>> 172.16.32.216
IP Virtual Server version 1.2.1 (size=4096)
Prot LocalAddress:Port Scheduler Flags
  -> RemoteAddress:Port           Forward Weight ActiveConn InActConn
TCP  10.254.0.1:443 rr
  -> 172.16.32.211:6443           Masq    1      0          0
  -> 172.16.32.212:6443           Masq    1      0          0
  -> 172.16.32.213:6443           Masq    1      0          0

```

可见所有通过 https 访问 K8S SVC kubernetes 的请求都转发到 kube-apiserver 节点的 6443 端口；

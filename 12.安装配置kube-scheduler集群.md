10.部署高可用 kube-scheduler 集群
本文档介绍部署高可用 kube-scheduler 集群的步骤。

该集群包含 3 个节点，启动后将通过竞争选举机制产生一个 leader 节点，其它节点为阻塞状态。当 leader 节点不可用后，剩余节点将再次进行选举产生新的 leader 节点，从而保证服务的可用性。

为保证通信安全，本文档先生成 x509 证书和私钥，kube-scheduler 在如下两种情况下使用该证书：

与 kube-apiserver 的安全端口通信;
在安全端口(https，10251) 输出 prometheus 格式的 metrics；

■创建和分发 kube-scheduler systemd unit 文件

```
cd /usr/local/kubernetes/work
cat > kube-scheduler.service.template <<EOF
[Unit]
Description=Kubernetes Scheduler
Documentation=https://github.com/kubernetes/kubernetes
After=network.target

[Service]
ExecStart=/usr/local/kubernetes/bin/kube-scheduler \
      --v=2 \
      --logtostderr=true \
      --bind-address=0.0.0.0 \
      --leader-elect=true \
      --kubeconfig=/etc/kubernetes/kube-scheduler.kubeconfig

Restart=always
RestartSec=10s

[Install]
WantedBy=multi-user.target
EOF
```

```
for (( i=0; i < 3; i++ ))
  do
    sed -e "s/##NODE_NAME##/${MASTER_NODE_NAMES[i]}/" -e "s/##NODE_IP##/${MASTER_NODE_IPS[i]}/" kube-scheduler.service.template > kube-scheduler-${MASTER_NODE_IPS[i]}.service 
  done
 ls kube-scheduler*.service
```




完整 unit 见 kube-scheduler.service。

分发 systemd unit 文件到所有 master 节点：

```
cd /usr/local/kubernetes/work
source /usr/local/kubernetes/bin/environment.sh
for node_ip in ${MASTER_NODE_IPS[@]}
  do
    echo ">>> ${node_ip}"
    scp kube-scheduler-${node_ip}.service root@${node_ip}:/etc/systemd/system/kube-scheduler.service
  done


```

■启动 kube-scheduler 服务

```
source /usr/local/kubernetes/bin/environment.sh
for node_ip in ${MASTER_NODE_IPS[@]}
  do
    echo ">>> ${node_ip}"
    ssh root@${node_ip} "mkdir -p ${K8S_DIR}/kube-scheduler"
    ssh root@${node_ip} "systemctl daemon-reload && systemctl enable kube-scheduler && systemctl restart kube-scheduler"
  done
```

●必须先创建日志目录；



■检查服务运行状态

```
source /usr/local/kubernetes/bin/environment.sh
for node_ip in ${MASTER_NODE_IPS[@]}
  do
    echo ">>> ${node_ip}"
    ssh root@${node_ip} "systemctl status kube-scheduler|grep Active"
  done
```

确保状态为 active (running)，否则查看日志，确认原因：

journalctl -u kube-scheduler
查看输出的 metric
注意：以下命令在 kube-scheduler 节点上执行。

kube-scheduler 监听 10251 端口，接收 http 请求：

[root@k8s1 work]# netstat -lnpt|grep kube-sche
tcp        0      0 172.16.22.1:10251       0.0.0.0:*               LISTEN      11713/kube-schedule 
tcp        0      0 172.16.22.1:10259       0.0.0.0:*               LISTEN      11713/kube-schedule 


注意：以下命令在 kube-scheduler 节点上执行。

kube-scheduler 监听 10251 和 10259 端口：

10251：接收 http 请求，非安全端口，不需要认证授权；
10259：接收 https 请求，安全端口，需要认证授权；
两个接口都对外提供 /metrics 和 /healthz 的访问。
$ curl -s http://172.16.22.1:10251/metrics |head
# HELP apiserver_audit_event_total Counter of audit events generated and sent to the audit backend.
# TYPE apiserver_audit_event_total counter
apiserver_audit_event_total 0
# HELP go_gc_duration_seconds A summary of the GC invocation durations.
# TYPE go_gc_duration_seconds summary
go_gc_duration_seconds{quantile="0"} 9.7715e-05
go_gc_duration_seconds{quantile="0.25"} 0.000107676
go_gc_duration_seconds{quantile="0.5"} 0.00017868
go_gc_duration_seconds{quantile="0.75"} 0.000262444
go_gc_duration_seconds{quantile="1"} 0.001205223

证书验证
curl -s --cacert /usr/local/k8s/work/ca.pem --cert /usr/local/k8s/work/admin.pem --key /usr/local/k8s/work/admin-key.pem https://172.16.22.1:10259/metrics |head
# HELP apiserver_audit_event_total Counter of audit events generated and sent to the audit backend.
# TYPE apiserver_audit_event_total counter
apiserver_audit_event_total 0
# HELP apiserver_audit_requests_rejected_total Counter of apiserver requests rejected due to an error in audit logging backend.
# TYPE apiserver_audit_requests_rejected_total counter
apiserver_audit_requests_rejected_total 0
# HELP apiserver_client_certificate_expiration_seconds Distribution of the remaining lifetime on the certificate used to authenticate a request.
# TYPE apiserver_client_certificate_expiration_seconds histogram
apiserver_client_certificate_expiration_seconds_bucket{le="0"} 0
apiserver_client_certificate_expiration_seconds_bucket{le="1800"} 0

测试 kube-scheduler 集群的高可用
随便找一个或两个 master 节点，停掉 kube-scheduler 服务，看其它节点是否获取了 leader 权限（systemd 日志）。

查看当前的 leader
$ kubectl get endpoints kube-scheduler --namespace=kube-system  -o yaml
apiVersion: v1
kind: Endpoints
metadata:
  annotations:
    control-plane.alpha.kubernetes.io/leader: '{"holderIdentity":"k8s-master1.frcloud.io_884f67ee-81fd-11e9-9bab-566fbe430006","leaseDurationSeconds":15,"acquireTime":"2019-05-29T10:36:08Z","renewTime":"2019-05-29T10:42:06Z","leaderTransitions":2}'
  creationTimestamp: "2019-05-29T09:26:24Z"
  name: kube-scheduler
  namespace: kube-system
  resourceVersion: "7574"
  selfLink: /api/v1/namespaces/kube-system/endpoints/kube-scheduler
  uid: d42eee39-81f3-11e9-9d09-566fbe430006
可见，当前的 leader 为 k8s-master1 节点。


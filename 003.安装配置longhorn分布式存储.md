# 003.安装配置longhorn分布式存储

<img alt="longhorn-logo" width="600" height="300" src="pics/longhorn.png">

longhorn 是免费的开源软件。最初由 Rancher Labs 开发，现在作为云原生计算基金会的孵化项目进行开发。

使用 Longhorn，您可以：

- 使用 Longhorn 卷作为 Kubernetes 集群中分布式有状态应用程序的持久存储
- 将您的块存储分区为 Longhorn 卷，以便您可以在有或没有云提供商的情况下使用 Kubernetes 卷
- 跨多个节点和数据中心复制块存储以提高可用性
- 将备份数据存储在 NFS 或 AWS S3 等外部存储中
- 创建跨集群灾难恢复卷，以便可以从第二个 Kubernetes 集群中的备份中快速恢复主 Kubernetes 集群中的数据
- 安排卷的重复快照，并将重复备份安排到 NFS 或与 S3 兼容的辅助存储
- 从备份还原卷
- 在不中断持久卷的情况下升级 Longhorn

## 架构及术语解释

Longhorn 系统包含用户部署组件（例如，Longhorn Manager(daemonset)、Driver Deployer、UI）和系统管理组件（例如，Instance Manager、Engine Image(daemonset)、CSI Driver(daemonset) 等）。

### longhorn-manager

使用 Operator 模式监听各种资源，包括不限于 Longhorn CRD 以及集群的 PV(C) 等资源，然后作出对应的响应。

Longhorn Manager Pod 作为 Kubernetes [DaemonSet 在 Longhorn 集群中的每个节点上运行。](https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/)它负责在 Kubernetes 集群中创建和管理卷，并处理来自 UI 或 Kubernetes 卷插件的 API 调用。它遵循 Kubernetes 控制器模式，有时也称为操作员模式。Longhorn Manager 与 Kubernetes API 服务器通信以创建新的 Longhorn 卷[CRD。](https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/)然后 Longhorn Manager 观察 API 服务器的响应，当它看到 Kubernetes API 服务器创建了一个新的 Longhorn 卷 CRD 时，Longhorn 管理器创建了一个新卷。

  当 Longhorn Manager 被要求创建一个卷时，它会在卷所连接的节点上创建一个 Longhorn Engine 实例，并在每个将放置副本的节点上创建一个副本。创建卷时，Longhorn Manager 创建 Longhorn Engine 微服务和每个卷的副本作为微服务。这些微服务共同构成了一个 Longhorn 卷。每个副本应放置在不同的节点或不同的磁盘上。副本应放置在不同的主机上以确保最大可用性。Longhorn 引擎始终与使用 Longhorn 卷的 Pod 在同一节点中运行。它在存储在多个节点上的多个副本之间同步复制卷。

```
longhorn-manager-2q6gz                         1/1     Running     0          53d   172.68.220.84   tsk8s4   <none>           <none>
longhorn-manager-6fz56                         1/1     Running     0          53d   172.68.14.22    tsk8s1   <none>           <none>
longhorn-manager-fm5qr                         1/1     Running     0          53d   172.68.62.94    tsk8s3   <none>           <none>
longhorn-manager-nd5sq                         1/1     Running     0          53d   172.68.244.22   tsk8s5   <none>           <none>
longhorn-manager-t7wwq                         1/1     Running     0          53d   172.68.101.90   tsk8s2   <none>           <none>
longhorn-manager-z25rc                         1/1     Running     0          53d   172.68.44.19    tsk8s6   <none>           <none>
```

### csi-attacher

是一个外部控制器，它监视由 controller-manager 创建的 VolumeAttachment 对象并将volume添加到node或者从node分离；

### csi-provisioner

是一个外部控制器，它监视由用户创建的 PersistentVolumeClaim 对象并且为用户创建或者删除；

### csi-resizer

是一个外部控制器，它监视 Kubernetes API server 以进行 PersistentVolumeClaim 修改并触发 CSI 调用以进行控制卷的扩展；

- 以上三个csi插件其实都是kubernetes-csi中的扩展插件，有兴趣可以访问：

https://github.com/kubernetes/community/blob/master/contributors/design-proposals/storage/container-storage-interface.md

### engine-image-ei

感觉这个pod类似是来控制镜像的，不过也确实如此，在longhorn的UI中确实有提到；

### Instance-manager-x

Longhorn Engine 管理 Volume 的多个副本，Engine 与 副本实质都会是一个单独的 Linux 进程运行,在卷所连接的节点上创建一个 Longhorn Engine 实例并产生一个engine服务进程



```
engine-image-ei-766a591b-444tc                 1/1     Running     0          53d   172.68.220.85   tsk8s4   <none>           <none>
engine-image-ei-766a591b-7bc5l                 1/1     Running     0          53d   172.68.62.97    tsk8s3   <none>           <none>
engine-image-ei-766a591b-8gjkb                 1/1     Running     0          53d   172.68.244.25   tsk8s5   <none>           <none>
engine-image-ei-766a591b-f9brw                 1/1     Running     0          53d   172.68.14.24    tsk8s1   <none>           <none>
engine-image-ei-766a591b-mhd6n                 1/1     Running     0          53d   172.68.44.22    tsk8s6   <none>           <none>
engine-image-ei-766a591b-qxbg2                 1/1     Running     0          53d   172.68.101.94   tsk8s2   <none>           <none>
```



### longhorn-csi-plugin

Longhorn CSI Plugin 负责接收标准 CSI 容器发起的 gRPC 调用，Longhorn CSI Plugin 将 Kubernetes CSI gRPC 调用转换为自己的 Longhorn API 调用，并将其转发到 Longhorn Manager 控制平面；

Longhorn 通过[CSI 插件在 Kubernetes 中进行管理。](https://kubernetes-csi.github.io/docs/)这样可以轻松安装 Longhorn 插件。

Kubernetes CSI 插件调用 Longhorn 来创建卷，从而为 Kubernetes 工作负载创建持久数据。CSI 插件使您能够创建、删除、附加、分离、挂载卷以及拍摄卷的快照。Longhorn 提供的所有其他功能都是通过 Longhorn UI 实现的。

Kubernetes 集群内部使用 CSI 接口与 Longhorn CSI 插件进行通信。Longhorn CSI 插件使用 Longhorn API 与 Longhorn Manager 通信。

Longhorn 确实利用了 iSCSI，因此可能需要对节点进行额外配置。这可能包括安装 open-iscsi 或 iscsiadm，具体取决于发行版。

```
longhorn-csi-plugin-88grg                      2/2     Running     0          53d   172.68.44.25    tsk8s6   <none>           <none>
longhorn-csi-plugin-fxqjp                      2/2     Running     0          53d   172.68.101.98   tsk8s2   <none>           <none>
longhorn-csi-plugin-l4qrt                      2/2     Running     0          53d   172.68.244.30   tsk8s5   <none>           <none>
longhorn-csi-plugin-lvbqd                      2/2     Running     0          53d   172.68.62.102   tsk8s3   <none>           <none>
longhorn-csi-plugin-tkds8                      2/2     Running     0          53d   172.68.220.90   tsk8s4   <none>           <none>
longhorn-csi-plugin-tvbr9                      2/2     Running     0          53d   172.68.14.28    tsk8s1   <none>           <none>
```



```
csi-attacher-dcb85d774-dfpdq                   1/1     Running     0          53d   172.68.44.23    tsk8s6   <none>           <none>
csi-attacher-dcb85d774-h299h                   1/1     Running     0          53d   172.68.244.27   tsk8s5   <none>           <none>
csi-attacher-dcb85d774-s8qpd                   1/1     Running     0          53d   172.68.220.88   tsk8s4   <none>           <none>
csi-provisioner-5d8dd96b57-4nvs2               1/1     Running     0          53d   172.68.244.28   tsk8s5   <none>           <none>
csi-provisioner-5d8dd96b57-cmbrj               1/1     Running     0          53d   172.68.44.24    tsk8s6   <none>           <none>
csi-provisioner-5d8dd96b57-ktm4d               1/1     Running     0          53d   172.68.220.89   tsk8s4   <none>           <none>
csi-resizer-7c5bb5fd65-gnh59                   1/1     Running     0          53d   172.68.244.29   tsk8s5   <none>           <none>
csi-resizer-7c5bb5fd65-nx6fn                   1/1     Running     0          53d   172.68.14.26    tsk8s1   <none>           <none>
csi-resizer-7c5bb5fd65-wdsvj                   1/1     Running     0          53d   172.68.62.100   tsk8s3   <none>           <none>
csi-snapshotter-5586bc7c79-d5mh6               1/1     Running     0          53d   172.68.101.97   tsk8s2   <none>           <none>
csi-snapshotter-5586bc7c79-h67l7               1/1     Running     0          53d   172.68.14.27    tsk8s1   <none>           <none>
csi-snapshotter-5586bc7c79-j5cr7               1/1     Running     0          53d   172.68.62.101   tsk8s3   <none>           <none>
```

- csi-XXX pod是由longhorn-csi-plugin fock出来，在每个节点产生一个服务进程



### longhorn-driver-deployer

longhorn-driver-deployer 实际上也是 longhorn-manager 镜像启动，只不过启动后会沟通 longhorn-manager HTTP API，然后创建所有 CSI 相关容器，包括 csi-provisioner、csi-snapshotter、longhorn-csi-plugin 等。

Longhorn CSI 驱动程序获取块设备，对其进行格式化，然后将其安装在节点上。然后[kubelet](https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet/)将设备绑定挂载到 Kubernetes Pod 中。这允许 Pod 访问 Longhorn 卷。

所需的 Kubernetes CSI 驱动程序映像将由 longhorn 驱动程序部署程序自动部署。

```
longhorn-driver-deployer-595565dc-ztq4f        1/1     Running     0          53d   172.68.101.91   tsk8s2   <none>           <none>
```



### longhorn-ui

Longhorn UI 向外暴露一个 Dashboard，并用过 Longhorn API 与 Longhorn Manager 控制平面交互；

### 存储快照

快照存储在本地，作为卷的每个副本的一部分。它们存储在 Kubernetes 集群内节点的磁盘上。快照与主机物理磁盘上的卷数据存储在同一位置。

###  灾难恢复卷

灾难恢复 (DR) 卷是一种特殊卷，可将数据存储在备份集群中，以防整个主集群出现故障。DR 卷用于提高 Longhorn 卷的弹性。

### 备份

备份是备份存储中的一个对象。备份存储包含备份卷，每个备份卷可能包含同一原始卷的多个备份。

可以从快照创建备份。它们包含创建快照时卷的状态，但它们不包含快照，因此它们不包含卷数据更改的历史记录。虽然备份由 2 MB 文件组成，但快照可以达到 TB。

备份由对象存储中的 2 MB 块组成



## 安装前的镜像清单

### 指定 Longhorn 镜像和标签：

```yaml
  longhorn:
    engine:
      repository: longhornio/longhorn-engine
      tag: v1.3.0
    manager:
      repository: longhornio/longhorn-manager
      tag: v1.3.0
    ui:
      repository: longhornio/longhorn-ui
      tag: v1.3.0
    instanceManager:
      repository: longhornio/longhorn-instance-manager
      tag: v1_20220611
    shareManager:
      repository: longhornio/longhorn-share-manager
      tag: v1_20220531
    backingImageManager:
      repository: longhornio/backing-image-manager
      tag: v3_20220609

```

### 指定 CSI Driver 组件镜像和标签：

```yaml
  csi:
    attacher:
      repository: longhornio/csi-attacher
      tag: v3.4.0
    provisioner:
      repository: longhornio/csi-provisioner
      tag: v2.1.2
    nodeDriverRegistrar:
      repository: longhornio/csi-node-driver-registrar
      tag: v2.5.0
    resizer:
      repository: longhornio/csi-resizer
      tag: v1.2.0
    snapshotter:
      repository: longhornio/csi-snapshotter
      tag: v3.0.3
  pullPolicy: IfNotPresent

```



## 使用helm安装longhorn 

### 安装open-iscsi(所有节点执行)

```fallback
yum --setopt=tsflags=noscripts install iscsi-initiator-utils
echo "InitiatorName=$(/sbin/iscsi-iname)" > /etc/iscsi/initiatorname.iscsi
systemctl enable iscsid
systemctl start iscsid
```



### 将kubernetes 节点打上Label(分配 longhorn磁盘的节点)

```
kubectl label node tsk8s1 kubernetes.io/stor=data
kubectl label node tsk8s2 kubernetes.io/stor=data
kubectl label node tsk8s3 kubernetes.io/stor=data
```

### 配置 longhorn Repo 

```
helm repo add longhorn https://charts.longhorn.io
helm repo update
```

## 生成longhorn这个chart的可定制配置

```
curl -Lo values-longhorn1.3.1.yaml https://raw.githubusercontent.com/longhorn/charts/master/charts/longhorn/values.yaml
```

## 编辑longhorn_values.yaml 值

```
# Default values for longhorn.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.
global:
  cattle:
    systemDefaultRegistry: ""
    windowsCluster:
      # Enable this to allow Longhorn to run on the Rancher deployed Windows cluster
      enabled: false
      # Tolerate Linux node taint
      tolerations:
      - key: "cattle.io/os"
        value: "linux"
        effect: "NoSchedule"
        operator: "Equal"
      # Select Linux nodes
      nodeSelector:
        kubernetes.io/os: "linux"
      # Recognize toleration and node selector for Longhorn run-time created components
      defaultSetting:
        taintToleration: cattle.io/os=linux:NoSchedule
        systemManagedComponentsNodeSelector: kubernetes.io/os:linux

image:
  longhorn:
    engine:
      repository: longhornio/longhorn-engine
      tag: master-head
    manager:
      repository: longhornio/longhorn-manager
      tag: master-head
    ui:
      repository: longhornio/longhorn-ui
      tag: master-head
    instanceManager:
      repository: longhornio/longhorn-instance-manager
      tag: v1_20220920
    shareManager:
      repository: longhornio/longhorn-share-manager
      tag: v1_20220914
    backingImageManager:
      repository: longhornio/backing-image-manager
      tag: v3_20220808
  csi:
    attacher:
      repository: longhornio/csi-attacher
      tag: v3.4.0
    provisioner:
      repository: longhornio/csi-provisioner
      tag: v2.1.2
    nodeDriverRegistrar:
      repository: longhornio/csi-node-driver-registrar
      tag: v2.5.0
    resizer:
      repository: longhornio/csi-resizer
      tag: v1.2.0
    snapshotter:
      repository: longhornio/csi-snapshotter
      tag: v3.0.3
  pullPolicy: IfNotPresent

service:
  ui:
    type: NodePort
    nodePort: 30809
  manager:
    type: ClusterIP
    nodePort: ""
    loadBalancerIP: ""
    loadBalancerSourceRanges: ""

persistence:
  defaultClass: true
  defaultFsType: ext4
  defaultClassReplicaCount: 3
  defaultDataLocality: disabled # best-effort otherwise
  reclaimPolicy: Delete
  migratable: false
  recurringJobSelector:
    enable: false
    jobList: []
  backingImage:
    enable: false
    name: ~
    dataSourceType: ~
    dataSourceParameters: ~
    expectedChecksum: ~
  defaultNodeSelector:
    enable: false # disable by default
    selector: []

csi:
  kubeletRootDir: ~
  attacherReplicaCount: ~
  provisionerReplicaCount: ~
  resizerReplicaCount: ~
  snapshotterReplicaCount: ~

defaultSettings:
  backupTarget: ~
  backupTargetCredentialSecret: ~
  allowRecurringJobWhileVolumeDetached: ~
  createDefaultDiskLabeledNodes: ~
  defaultDataPath: ~
  defaultDataLocality: ~
  replicaSoftAntiAffinity: ~
  replicaAutoBalance: ~
  storageOverProvisioningPercentage: ~
  storageMinimalAvailablePercentage: ~
  upgradeChecker: ~
  defaultReplicaCount: 3
  defaultLonghornStaticStorageClass: ~
  backupstorePollInterval: ~
  failedBackupTTL: ~
  recurringSuccessfulJobsHistoryLimit: ~
  recurringFailedJobsHistoryLimit: ~
  taintToleration: ~
  systemManagedComponentsNodeSelector: "kubernetes.io/role: k8s-control"
  priorityClass: ~
  autoSalvage: ~
  autoDeletePodWhenVolumeDetachedUnexpectedly: ~
  disableSchedulingOnCordonedNode: ~
  replicaZoneSoftAntiAffinity: ~
  nodeDownPodDeletionPolicy: ~
  allowNodeDrainWithLastHealthyReplica: ~
  mkfsExt4Parameters: ~
  disableReplicaRebuild: ~
  replicaReplenishmentWaitInterval: ~
  concurrentReplicaRebuildPerNodeLimit: ~
  disableRevisionCounter: ~
  systemManagedPodsImagePullPolicy: ~
  allowVolumeCreationWithDegradedAvailability: ~
  autoCleanupSystemGeneratedSnapshot: ~
  concurrentAutomaticEngineUpgradePerNodeLimit: ~
  backingImageCleanupWaitInterval: ~
  backingImageRecoveryWaitInterval: ~
  guaranteedEngineManagerCPU: ~
  guaranteedReplicaManagerCPU: ~
  kubernetesClusterAutoscalerEnabled: ~
  orphanAutoDeletion: ~
  storageNetwork: ~
privateRegistry:
  createSecret: ~
  registryUrl: ~
  registryUser: ~
  registryPasswd: ~
  registrySecret: ~

longhornManager:
  log:
    ## Allowed values are `plain` or `json`.
    format: plain
  priorityClass: ~
  tolerations: []
  ## If you want to set tolerations for Longhorn Manager DaemonSet, delete the `[]` in the line above
  ## and uncomment this example block
  # - key: "key"
  #   operator: "Equal"
  #   value: "value"
  #   effect: "NoSchedule"
  nodeSelector: {}
  ## If you want to set node selector for Longhorn Manager DaemonSet, delete the `{}` in the line above
  ## and uncomment this example block
  #  label-key1: "label-value1"
  #  label-key2: "label-value2"
  serviceAnnotations: {}
  ## If you want to set annotations for the Longhorn Manager service, delete the `{}` in the line above
  ## and uncomment this example block
  #  annotation-key1: "annotation-value1"
  #  annotation-key2: "annotation-value2"

longhornDriver:
  priorityClass: ~
  tolerations: []
  ## If you want to set tolerations for Longhorn Driver Deployer Deployment, delete the `[]` in the line above
  ## and uncomment this example block
  # - key: "key"
  #   operator: "Equal"
  #   value: "value"
  #   effect: "NoSchedule"
  #nodeSelector: {}
  nodeSelector:
    kubernetes.io/role: 'k8s-control'
  ## If you want to set node selector for Longhorn Driver Deployer Deployment, delete the `{}` in the line above
  ## and uncomment this example block
  #  label-key1: "label-value1"
  #  label-key2: "label-value2"

longhornUI:
  replicas: 1
  priorityClass: ~
  tolerations: []
  ## If you want to set tolerations for Longhorn UI Deployment, delete the `[]` in the line above
  ## and uncomment this example block
  # - key: "key"
  #   operator: "Equal"
  #   value: "value"
  #   effect: "NoSchedule"
  #nodeSelector: {}
  nodeSelector:
    kubernetes.io/role: 'k8s-control'
  ## If you want to set node selector for Longhorn UI Deployment, delete the `{}` in the line above
  ## and uncomment this example block
  #  label-key1: "label-value1"
  #  label-key2: "label-value2"

ingress:
  ## Set to true to enable ingress record generation
  enabled: false

  ## Add ingressClassName to the Ingress
  ## Can replace the kubernetes.io/ingress.class annotation on v1.18+
  ingressClassName: ~

  host: sslip.io

  ## Set this to true in order to enable TLS on the ingress record
  tls: false

  ## Enable this in order to enable that the backend service will be connected at port 443
  secureBackends: false

  ## If TLS is set to true, you must declare what secret will store the key/certificate for TLS
  tlsSecret: longhorn.local-tls

  ## If ingress is enabled you can set the default ingress path
  ## then you can access the UI by using the following full path {{host}}+{{path}}
  path: /

  ## Ingress annotations done as key:value pairs
  ## If you're using kube-lego, you will want to add:
  ## kubernetes.io/tls-acme: true
  ##
  ## For a full list of possible ingress annotations, please see
  ## ref: https://github.com/kubernetes/ingress-nginx/blob/master/docs/annotations.md
  ##
  ## If tls is set to true, annotation ingress.kubernetes.io/secure-backends: "true" will automatically be set
  annotations:
  #  kubernetes.io/ingress.class: nginx
  #  kubernetes.io/tls-acme: true

  secrets:
  ## If you're providing your own certificates, please use this to add the certificates as secrets
  ## key and certificate should start with -----BEGIN CERTIFICATE----- or
  ## -----BEGIN RSA PRIVATE KEY-----
  ##
  ## name should line up with a tlsSecret set further up
  ## If you're using kube-lego, this is unneeded, as it will create the secret for you if it is not set
  ##
  ## It is also possible to create and manage the certificates outside of this helm chart
  ## Please see README.md for more information
  # - name: longhorn.local-tls
  #   key:
  #   certificate:

# Configure a pod security policy in the Longhorn namespace to allow privileged pods
enablePSP: true

## Specify override namespace, specifically this is useful for using longhorn as sub-chart
## and its release namespace is not the `longhorn-system`
namespaceOverride: ""

# Annotations to add to the Longhorn Manager DaemonSet Pods. Optional.
annotations: {}

serviceAccount:
  # Annotations to add to the service account
  annotations: {}

```



##  修改以上配置，为 Longhorn 设置节点选择器

Longhorn 系统包含用户部署的组件（例如，Manager、Driver Deployer、UI）和系统管理的组件（例如，Instance Manager、Engine Image、CSI Driver 等）。您需要为这两种类型的组件设置节点选择器。请参阅下面的更多详细信息。



## helm定制安装longhorn chart

```
helm install longhorn longhorn/longhorn   --namespace longhorn-system   --create-namespace   --values values-longhorn1.3.1.yaml
```

- 要启用对 Longhorn UI 的访问，您需要设置一个 Ingress 控制器。默认情况下不启用对 Longhorn UI 的身份验证。有关使用基本身份验证创建 NGINX Ingress 控制器的信息，请参阅[本节。](https://longhorn.io/docs/1.3.1/deploy/accessing-the-ui/longhorn-ingress)

- Please wait a few minutes for other Longhorn components such as CSI deployments, Engine Images, and Instance Managers to be initialized.

   

## 创建各种卷

### Block Volume

```
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: longhorn-block-vol
spec:
  accessModes:
    - ReadWriteOnce
  volumeMode: Block
  storageClassName: longhorn
  resources:
    requests:
      storage: 2Gi
---
apiVersion: v1
kind: Pod
metadata:
  name: block-volume-test
  namespace: default
spec:
  containers:
    - name: block-volume-test
      image: nginx:stable-alpine
      imagePullPolicy: IfNotPresent
      volumeDevices:
        - devicePath: /dev/longhorn/testblk
          name: block-vol
      ports:
        - containerPort: 80
  volumes:
    - name: block-vol
      persistentVolumeClaim:
        claimName: longhorn-block-vol
```


-----------------------------------------------------------------------

在容器内部产生一个/dev/longhorn/testblk的块设备，未被使用

### Pod with PersistentVolumeClaim

```
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: longhorn-volv-pvc
spec:
  accessModes:
    - ReadWriteOnce
  storageClassName: longhorn
  resources:
    requests:
      storage: 2Gi
---
apiVersion: v1
kind: Pod
metadata:
  name: volume-test
  namespace: default
spec:
  restartPolicy: Always
  containers:
  - name: volume-test
    image: nginx:stable-alpine
    imagePullPolicy: IfNotPresent
    livenessProbe:
      exec:
        command:
          - ls
          - /data/lost+found
      initialDelaySeconds: 5
      periodSeconds: 5
    volumeMounts:
    - name: volv
      mountPath: /data
    ports:
    - containerPort: 80
  volumes:
  - name: volv
    persistentVolumeClaim:
      claimName: longhorn-volv-pvc
```





参考文件：

https://longhorn.io/

https://github.com/longhorn/longhorn

https://www.modb.pro/db/465303

https://kubernetes.io/docs/concepts/storage/persistent-volumes/#raw-block-volume-support
